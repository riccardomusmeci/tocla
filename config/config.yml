###### Datamodule ######
data:
  class_map: {
    0: [class_a, class_b],
    1: [class_c, class_d]
  }
  imbalanced: true                            # if dataset is imbalanced
  max_samples_per_class: null                 # max number of samplese for each class in the dataset
  random_samples: true                        # if max_samples_per_class is specified it selects random samples
  batch_size: 64                              # batch size
  shuffle: true                               # if shuffling dataset
  num_workers: 5                              # num threads for data loaders
  pin_memory: true                            # data loader pin memory
  drop_last: false                            # data loader drop last batch if does not match batch size
  persistent_workers: true                    # data loader persistent worker

trainer:
  max_epochs: 200                             # max number of epochs
  device: mps                                 # device
  check_val_every_n_epoch: 1                  # check validation metric stats every n epochs
  check_train_every_n_iter: 2                 # print train stats every n iter
  gradient_clip_val: null                     # gradient clip value
  gradient_clip_algorithm: norm               # if you're using SAM optim, make sure to use gradient_clip_algorithm set to 'norm'

###### Model ######
model:  
  model_name: efficientnet_b0                 # timm model name
  pretrained: true                            # imagenet pretrained weights

###### Loss ###### 
loss:                                           
  name: xent                                  # name of the loss criterion to use
  weight: auto                                # if auto computes automatically the class weights
  label_smoothing: null                       # [NOT WORKING WITH MPS] label smoothing factor -> to prevent model being overconfident (calibration) - 
                                              
###### Optimizer ######  
optimizer:
  name: sam                                   # optimization algorithm (sgd, adam, adamw)
  base_optimizer: sgd                         # base optimizer per sam
  rho: 0.05                                   # rho parameter for SAM
  lr: 0.001                                   # base learning rate at the start of the training
  adaptive: true                              # True if you want to use the Adaptive SAM.
  weight_decay: 0.0001                        # l2 weigth decay
  momentum: 0.9                               # sgd momentum           
  bn_to_zero: true                            # enabling and disabling BN in the two optimizer steps

###### LR Scheduler ######
lr_scheduler:
  name: cosine_restarts                       # lr_scheduler name
  T_0: 5                                      # when to start cosine scheduling
  T_mult: 2                                   # multiplicative factor between restarting
  eta_min: 0                                  # min lr to reach

######## Data Augmentation ###########
transform: 
  img_size: 224                               # input image size
  mean: [0.485, 0.456, 0.406]                 # ImageNet mean normalization ([0.485, 0.456, 0.406])
  std: [0.229, 0.224, 0.225]                  # ImageNet std normalization ([0.229, 0.224, 0.225])
  # Flips
  h_flip_p: 0.5                               # HorizontalFlip transformation probabilty
  v_flip_p: 0.5                               # VerticalFlip transformation probability
  # ShiftScaleRotate
  ssr_p: 0.3                                  # ShiftScaleRotate probability
  ssr_border_mode: 0                          # ShiftScaleRotate border mode
  ssr_rotate_limit: 20                        # ShiftScaleRotate rotate limit
  ssr_scale_limit: 0.5                        # ShiftScaleRotate scale limit
  ssr_shift_limit: 0.1                        # ShiftScaleRotate shift limit
  ssr_value: 0                                # ShiftScaleRotate value
  # GaussianNoise
  gn_var_limit: [10, 50]                      # GaussianNoise variance limit
  gn_mean: 0                                  # GaussianNoise mean
  gn_p: 0                                     # GaussianNoise probability
  # Crop
  crop_p: 0.3                                 # Crop probabilty  
  # OneOf:
  #   1) Sharpen + Blur + MotionBlur
  #   2) RandomBrightnessContrast + HueSaturationValue
  one_of_p: 0.66                              # OneOf transformation probability
  sharpen_alpha: 0.5                          # Sharpen alpha
  sharpen_lightness: 0.5                      # Sharpen lightness
  sharpen_p: 0.5                              # Sharpen probability
  blur_limit: [3, 3]                          # Blur limit
  blur_p: 0.25                                # Blur probability
  mblur_limit: [3, 3]                         # MotionBlur blur limit
  mblur_p: 0.25                               # MotionBlur blur probability
  rbc_brightness_limit: 0.1                   # RandomBrightnessContrast brightness limit
  rbc_contrast_limit: 0.1                     # RandomBrightnessContrast contrast limit
  rbc_p: 0.5                                  # RandomBrightnessContrast contrast probability
  hsv_sat_shift_limit: 10                     # HueSaturationValue saturation limit
  hsv_hue_shift_limit: 10                     # HueSaturationValue hue limit
  hsv_val_shift_limit: 10                     # HueSaturationValue value limit
  hsv_p: .5                                   # HueSaturationValue probability

##### Checkpoint #####
checkpoint:
  monitor: f1                                 # which metric to monitor ["accuracy", "loss/val", "loss/train", "recall", "calibration_error", "precision"]
  mode: max                                   # monitor mode
  save_top_k: 5                               # save top k pth models
  patience: 5                                 # number of epochs to wait before stopping training
